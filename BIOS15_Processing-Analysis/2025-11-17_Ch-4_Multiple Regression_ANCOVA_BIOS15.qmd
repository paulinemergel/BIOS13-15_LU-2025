---
title: BIOS 15: Processing and Analysis of Biological Data
subtitle: Chapter 4. The Linear Model III: Multiple regression and Analysis of Covariance
author: Øystein H. Opedal (author), Pauline Mergel (edits/exercises)
date: 2025-11-17
output: pdf_document
fig_caption: yes
pandoc_args: [--wrap=none]
---

```{r setup, include=T}
knitr::opts_chunk$set(echo = TRUE)

## Sets global chunk options for all R code chunks in the document

# echo = TRUE tells knitr to show the R code in the rendered output.
# So every code chunk will display its code unless you override it locally with echo = FALSE

# include = FALSE (in the chunk header) means:
# The setup chunk itself will not appear in the final output — neither its code nor its results are shown.
# But the code does run, so it can set options like this.
```

## Packages

```{r}
#| include: false
#| warning: false

## Packages
library(tidyverse)
```

# Multiple Regression

Linear models are easily extendable to multiple predictor variables. If there are several continuous predictors, the analysis is called a multiple-regression analysis. Multiple regression has some very useful properties. For example, the parameter estimates represent the *marginal effect* (= direct effect) of each predictor, that is the effect of the predictor when all other variables in the model are held constant at their mean. This allows us to evaluate the independent effects of several, potentially correlated, variables (asking for example which have the stronger effect on the response variable), or to 'control for' some nuisance variables (say, sampling effort).

We can also include a mixture of continuous and categorical variables in a model, in which case we technically perform an analysis of covariance (more below).

The following code simulates data with two correlated predictor variables and fits a multiple-regression model.

```{r}
set.seed(187)
x1 = rnorm(200, 10, 2)
x2 = 0.5*x1 + rnorm(200, 0, 4)
y = 0.7*x1 + 2.2*x2 + rnorm(200, 0, 4)

m = lm(y~x1+x2)

coefs = summary(m)$coef

# [plain text] \newpage
```

```{r}
summary(m)
```

First, note that the coefficient of determination ($r^2$) of the model is 0.868, which means as before that 86.8% of the variance in $y$ is explained. As before, we can see why this is the case by computing the variance in the predicted values $\hat{y}$, $V(\hat{y}) = V(X\beta)$, and then divide this by the total variance in the response variable $V(y)$.

What exactly is $\hat{y}$?

Think of regression as drawing a line (or hyperplane) through your data that best fits the pattern.

Ŷ is what the model thinks Y should be, given certain values of the predictors.

-   **Y** = actual data points

-   **Ŷ** = points on the regression line (what the model predicts/model's expected outcome)

The difference between actual and predicted is the **residual**: **e = Y − Ŷ**. Residuals tell you how good your model is.

```{r}
# coefs == values of the estimate for the intercept, x1, and x3
# see: coefs

y_hat = coefs[1,1] + coefs[2,1]*x1 + coefs[3,1]*x2 # returns n = 200 values

y_hat_var <- var(y_hat)
y_var <- var(y)

y_total_var_r2 <- y_hat_var/y_var ### = r-squared
```

This is the total variance explained by the model. Now what about the **variance explained by each of the predictors** $x_1$ **and** $x_2$? To compute the predicted values associated only **with** $x_1$**, we keep** $x_2$ **constant at its mean**, and *vice versa* for the variance associated with $x_2$.

```{r}
y_hat_x1 = coefs[1,1] + coefs[2,1]*x1 + coefs[3,1]*mean(x2) # MEAN of x2
y_hat_x1_var <- var(y_hat_x1)
y_hat_x1_var_tot <- var(y_hat_x1)/var(y)
```

```{r}
y_hat_x2 = coefs[1,1] + coefs[2,1]*mean(x1) + coefs[3,1]*x2 # MEAN of x1
y_hat_x2_var <- var(y_hat_x2)
y_hat_x2_var_tot <- var(y_hat_x2)/var(y)
```

Now, we compare the sum of the variance explained by $x_1$ and $x_2$ to the total variance in $y$.

```{r}
y_hat_var # total variance in y
y_hat_x1plusx2_var <- y_hat_x1_var + y_hat_x2_var
y_hat_x1plusx2_var # sum of the variance explained by x1 + x2
```

So, what happened to the last few percent of the variance? Recall that

$Var(x+y) = Var(x) + Var(y) + 2Cov(x,y)$.

```{r}
#var(y_hat1) + var(y_hat2) + 2*cov(y_hat1, y_hat2)
xplusy_var <- y_hat_x1_var + y_hat_x2_var + 2*(cov(y_hat_x1, y_hat_x2))
xplusy_var # remaining variance
```

As before, we can also do this by computing $V(x) = \beta_x^2\sigma_x^2$.

```{r}
x_var <- coefs[2,1]^2*var(x1)
x_var
```

```{r}
var_tomerge <- c(y_hat_var, y_var, y_hat_x1plusx2_var, y_total_var_r2, y_hat_x1_var, y_hat_x1_var_tot, y_hat_x2_var, y_hat_x2_var_tot , xplusy_var, x_var)

results <- mget(var_tomerge, ifnotfound = NA) |>       # <- returns NA for missing variables
  map(~ as.numeric(.x)) |>                 # ensure numeric values
  enframe(name = name, value = value) |>
  unnest(value) |>
  mutate(
    value = round(value, 1),              # round to 1 decimal
    #type  = regression_output           # add a second column
  )
results
```

To include the covariance between the predictors, we can do this in matrix notation $V(\hat{y}) = \mathbf{\hat{\beta^T}S\hat{\beta}}$, where $\hat{\beta}$ is a vector of parameter estimates (slopes), $\mathbf{S}$ is the covariance matrix for the predictors, and $^T$ means transposition. Recall the `R` matrix multiplication operator `%*%`.

```{r}
t(coefs[2:3,1]) %*% cov(cbind(x1,x2)) %*% coefs[2:3,1]
```

This latter approach is the most general, as it extends to any number of predictors in the model. Note that we could, for example, compute the variance explained by a subset of the predictors by specifying the correct vector of $\beta$ coefficients and their corresponding variance-covariance matrix. This is useful if we had, say, 3 variables related to climate and 3 other variables related to local land-use, and wanted to know how these sets each explain variance in some variable (say, the size of pine trees).

## Z-transformation

This procedure also hints at a method for obtaining parameter estimates that directly reflect the strength of the effects of each predictor. If all variables had the same variance, then the variance explained would be directly proportional to the regression slope. The most common way to standardize predictor variables is to scale them to zero mean and unit variance, a so-called $z$-transform

$z = \frac{x-\bar{x}}{\sigma(x)}$

The resulting variable will have a mean of zero and a standard deviation (and variance) of one (remember to check that this is indeed the case).

```{r}
x1_z = (x1 - mean(x1))/sd(x1)
x2_z = (x2 - mean(x2))/sd(x2)

m = lm(y ~ x1_z + x2_z)
summary(m)
```

Note that the model fit (e.g. the $r^2$) has not changed, but the parameter estimates have. First, the intercept can now be interpreted as the mean of $y$, because it represents the value of $y$ when both predictors have a value of 0 (i.e. their mean after the $z$-transform). This effect can be obtained also by mean-centering the variables without scaling them to a standard deviation of 1.

Second, the slopes now have units of standard deviations, i.e. they describe the change in $y$ per standard deviation change in each predictor. This shows directly that the predictor $x_2$ explains more variance in $y$ than does $x_1$.

Another useful transformation could be a natural log-transform, or similarly mean-scaling, which would give the slopes units of means, and allow interpreting the change in $y$ per percent change in $x$. These proportional slopes are technically called *elasticities*.

```{r}
x1_m = (x1 - mean(x1))/mean(x1)
x2_m = (x2 - mean(x2))/mean(x2)

summary(lm(y ~ x1_m + x2_m))
```

## Multicollinearity

When we have several predictors that are strongly correlated with each other, it becomes difficult to estimate their independent effects. A rule of thumb is that such ***multicollinearity*** **becomes a potential problem when the correlation between the predictors is greater than 0.6 or 0.7**. One way of assessing the degree of multicollinearity is to compute ***variance inflation factors***, defined as

$VIF_i = \frac{1}{1-r^2_i}$

where the $r^2$ is from a regression of covariate $i$ on the other covariates included in the model. For our example model, the variance inflation factor for covariate $x_1$ is thus

```{r}
m1 = lm(x1~x2)
r2 = summary(m1)$r.squared
1/(1-r2)
```

This is very low, because the two predictors are not strongly correlated. Rules of thumb for what constitutes **severe variance inflation range from** $VIF>3$ **to** $VIF>10$. When this occurs, the parameter estimates become associated with excessive variance and are thus less reliable. In these cases it may be good to simplify the model by removing some of the correlated predictors, especially if there are several predictors that essentially represent the same property (e.g. multiple measures of body size). If the effects of the correlated predictors are of specific interest, it can also make sense to fit alternative models including each of the candidate predictor, and compare estimates. If the 'best model' is desired, the choice among the predictor may be based on model selection techniques.

## Data exercise: multiple regression and variable selection

A common problem in biology is that we have a **large number of variables**, many of which could potentially predict variation in a response variable. Including a lot of predictors in the same model can lead to **problems with multicollinearity, with overfitting** (resulting in a model that explains a lot of variance but fails to predict independent test data), and **difficulties in interpretation**.

We will return in a later section to a more complete treatment of the **problem of model selection**, but for now let's consider two main approaches to choosing among a large set of potential variables. A **statistical** **approach** to the problem is to look for the simplest model that does a decent job in explaining variation in the data. This can be done e.g. by so-called backward selection of variables, which means that we start from a full (or saturated) model including all potential predictors, and then sequentially drop non-significant terms until all terms are statistically significant.

The problem with this approach is that it focuses on hypothesis testing over interpretation of effects. As we have discussed previously, a statistically significant hypothesis test does not necessarily mean that the effect is biologically important. One strategy for avoiding this fallacy is to **start from a well defined biological hypothesis** that can be formulated as a statistical model. The focus is then moved from statistical hypothesis testing to a **simpler task of parameter estimation and interpretation**.

### Alpine plant dataset

In the following exercise, we will try both approaches for the same dataset, and see if we end up with the same final model. The following dataset includes data on the local abundance of two alpine plant species, measured as the number of times the species was hit in a so-called pinpoint analysis, where 25 metal pins were passed vertically through the vegetation within a 25 $\times$ 25 cm plot. The data also include a number of environmental variables. The temperature variables are measured by micro-loggers placed just below the soil surface, so that the winter temperatures represent the temperature under any snowcover. Temperatures are measured in degrees Celsius, light intensity as the % of sunlight that reaches through the vegetation, snow cover in cm, altitude in m, and soil moisture in %.

### Tasks

1.  Start by exploring the data by extracting summaries and making basic graphs. Are there any obvious outliers? If so, consider removing them.

2.  Then, think about some possible hypotheses (models) that could explain the distribution of the plant species. Fit the models, evaluate the model fit (are the residuals roughly normally distributed?) and interpret the results.

3.  Then, do a backward selection in which you start from a saturated (full) model and sequentially drop the statistically least significant terms until all terms are statistically significant (P\<0.05).

4.  Do you end up with the same model?

### Data

```{r}
plants = read.csv(file=datasets/alpineplants.csv)
as.data.frame(plants)
nrow(plants) # 96 observations
names(plants) # 2x plant species, 10x environmental data
# => two plant species: Carex bigelowii and Thalictrum alpinum (units = local abundance)
# => 10x environmental variables (mean temp winter, max temp winter, snow, soil moisture, altitude, etc)

# Function the get the min and max values for each column:
min_max <- function(df) {
  summarise(df, across(where(is.numeric),
                       \(x) c(min = min(x, na.rm = TRUE),
                             max = max(x, na.rm = TRUE))))
}

min_max <- min_max(plants)
min_max
```

### Carex bigelowii (Starre Segge)

#### (1) Data exploration

```{r}
hist(plants$Carex.bigelowii, probability = T, main = "Histogram of Carex bigelowii")

hist(log(plants$Carex.bigelowii), probability = T, main = "Histogram of Carex bigelowii")

#----------------------------------------

## OUTLIERS
plants.carex.stats <- boxplot.stats(plants$Carex.bigelowii)
plants.carex.stats$out # 3 outliers w/ abundance values 8, 8, 6


## Outliers extended script
cb <- plants$Carex.bigelowii

# Calculate Q1, Q3, and IQR
Q1 <- quantile(cb, 0.25)
Q3 <- quantile(cb, 0.75)
IQR <- Q3 - Q1

# Define bounds
lower_bound <- Q1 - 1.5*IQR
upper_bound <- Q3 + 1.5*IQR

# Identify outliers
cb_outliers <- cb[cb < lower_bound | cb > upper_bound]
cb_outliers
#3x outlier
# biologically meaningful???

# Remove outliers
cb_no_outliers <- cb[cb >= lower_bound & cb <= upper_bound]
cb_no_outliers

plants
# 96-3 = 93 data points that are good (either way)

#----------------------------------------

## Plots to find Outliers

## !!!!!
pairs(plants)


##
plot_outliers <- function(data) {
  for(col in names(data)){
    boxplot(data[[col]], main = paste("Boxplot of", col), ylab = col)
  }
}
plot_outliers(plants)

plants$max_T_winter

# TIP: fit the model with and without the outliers and then check if parameter estimates change a lot

plants2 <- plants %>% filter(max_T_winter <= 1)
```

##### Full Model

```{r}
## FULL linear model with ALL predictors (11x)

# no (potential outliers removed)
# remember: maximum winter temp has one very high value
m.all <- lm(Carex.bigelowii ~ Thalictrum.alpinum + mean_T_winter + max_T_winter + min_T_winter + mean_T_winter + mean_T_summer + max_T_summer + min_T_summer + light + snow + soil_moist + altitude, data = plants)

par(mfrow = c(2,2))
plot(m.all) # does not look well

summary(m.all) # no good
```

#### (2) Biological hypotheses

Alpine dataset == cold-limited environment

-   **Response variable:** Carex bigelowii

-   **Predictors:**\
    Thalictrum alpinum\
    min, max, and mean winter temp\
    min, max, and mean summer temp\
    light\
    snow\
    soil moisture\
    altitude

-   unsure, if T. alpinum is a good predicor for C. bigelowii

-   minimum temperatures usually most critical (in comparison to max. and mean temp)

    -   winter temp more important than summer

-   What unit is light measured in?

-   Snow and soil moisture correlated?

##### Model A

*Carex bigelowii \~ Minimum Winter temperature*

```{r}
m.a <- lm(Carex.bigelowii ~ min_T_winter, data = plants)

par(mfrow = c(2,2))
plot(m.a)

summary(m.a)
```

```{r}
## Model A WITHOUT outliers for Carex 
plants.carex.stats <- boxplot.stats(plants$Carex.bigelowii)
plants.carex.stats$out # 3 outliers w/ abundance values 8, 8, 6

plants.a.2 <- plants %>% filter(Carex.bigelowii < 6)

m.a.2 <- lm(Carex.bigelowii ~ min_T_winter, data = plants.a.2)

par(mfrow = c(2,2))
plot(m.a.2) # residuals are still wonky and NOT normally distributed...

summary(m.a.2)
```

##### Model B

*Carex bigelowii \~ Altitude*

```{r}
m.b <- lm(Carex.bigelowii ~ altitude, data = plants)

par(mfrow = c(2,2))
plot(m.b)

summary(m.b)
```

##### Model C

*Carex bigelowii \~ Minimum Winter Temperature + Altitude*

```{r}
m.c <- lm(Carex.bigelowii ~ min_T_winter + altitude, data = plants)

par(mfrow = c(2,2))
plot(m.c) # residuals are still not normally distributed

summary(m.c) # Model A (without altitude) was better 
```

##### Model D

*Carex bigelowii \~ Minimum Winter temperature + Maximum Summer Temperature*

```{r}
m.d <- lm(Carex.bigelowii ~ min_T_winter + max_T_summer, data = plants)

par(mfrow = c(2,2))
plot(m.d)

summary(m.d)


## Z-TRANSFORMATION
# x1_z = (x1 - mean(x1))/sd(x1)
# x2_z = (x2 - mean(x2))/sd(x2)
# m = lm(y ~ x1_z + x2_z)

min_T_winter <- plants$min_T_winter
max_T_summer <- plants$max_T_summer
carex <- plants$Carex.bigelowii

min_T_winter_z = (min_T_winter - mean(min_T_winter))/sd(min_T_winter)
max_T_summer_z = (max_T_summer - mean(max_T_summer))/sd(max_T_summer)

m.d_z = lm(carex ~ min_T_winter_z + max_T_summer_z)
summary(m.d_z)

par(mfrow = c(2,2))
plot(m.d_z)

summary(m.d_z)
```

#### (3) Backwards, significance selection

```{r}
## FULL linear model with ALL predictors (11x)
## Predictors in order of *assumed* importance
m11 <- lm(Carex.bigelowii ~ 
              min_T_winter +
              max_T_summer +
              altitude +
              min_T_summer +
              max_T_winter + 
              snow + 
              soil_moist + 
              Thalictrum.alpinum +
              mean_T_winter +
              mean_T_summer,
              data = plants)
              
par(mfrow = c(2,2))
plot(m11)

summary(m11)
```

```{r}
m10 <- lm(Carex.bigelowii ~ 
              min_T_winter +
              max_T_summer +
              altitude +
              min_T_summer +
              max_T_winter + 
              snow + 
              soil_moist + 
              Thalictrum.alpinum +
              mean_T_winter,
              data = plants)
              
par(mfrow = c(2,2))
plot(m10)

summary(m10)
```

```{r}
m9 <- lm(Carex.bigelowii ~ 
              min_T_winter +
              max_T_summer +
              altitude +
              min_T_summer +
              max_T_winter + 
              snow + 
              soil_moist + 
              Thalictrum.alpinum,
              data = plants)
              
par(mfrow = c(2,2))
plot(m9)

summary(m9)
```

#### (4) Comparison

### QUESTIONS

-   (see (2) Biol. Hyp.: Model D)\
    Why does the z-transformation of the variables produce NAs?
-   (see (3))\
    Model 11 (full/all) TO Model reduced: what if I have significance in predictors "in the middle"? - should I "switch" something around?
-   

# Analysis of Covariance (ANCOVA)

Analysis of Covariance can be thought about as a combination of regression and analysis of variance. The simplest case is when we have a single continuous variable (covariate) and a single categorical predictor. An ANCOVA analysis can then be used to ask whether the slope of the regression differs between groups (levels of the categorical variable). A statistically supported interaction means that the slopes differ between groups, while a statistically supported main effect of groups means that the intercepts differ.

```{r}
set.seed(12)
x = rnorm(200, 50, 5)
gr = factor(c(rep(Male, 100), rep(Female, 100)))
y = -2 + 1.5*x + rnorm(200, 0, 5)
y[101:200] = 2 + 0.95*x[101:200] + rnorm(100, 0, 6)

```

```{r, fig.height=4, fig.width=4}
plot(x, y, pch=c(1,16)[as.numeric(gr)], las=1)
```

Just as for ANOVA analyses, we can extract two kinds of summaries for an ANCOVA analysis.

```{r}
m = lm(y~x*gr)
anova(m)
```

The `anova` function returns the familiar ANOVA table including the sums of squares, which allows us to assess which variables explain more variance in the response variable. To get the parameter estimates and standard errors, we call the `summary` function instead.

\newpage

```{r}
summary(m)
```

In this case the slope is steeper for males. Note that to obtain the slope for males, we have to sum the slope for females (`x`) and the interaction term (`x:grMale`). If we want to extract the male and female slopes and intercepts with their standard errors, we can reformulate the model by suppressing the global intercept.

```{r}
m2 = lm(y ~ -1 + gr + x:gr)
summary(m2)
```

Note that this is actually the same model as before, formulated in a different way. We can confirm this by checking that the *log likelihood* of the model remains unchanged.

```{r}
logLik(m)
logLik(m2)
```

## Data exercise: Interpreting linear-model analyses

Flowers are integrated phenotypes, which means that the different parts of the flowers are generally covarying with each other so that large flowers have e.g. both longer petals and longer sepals. Evolutionary botanists are interested in these patterns of covariation among floral parts, because they can affect for example the fit of flowers to their pollinators. We will work with a dataset on flower measurements from 9 natural populations in Costa Rica.

The traits are

-   ASD: anther-stigma distance ($mm$)
-   GAD: gland-anther distance ($mm$)
-   GSD: gland-stigma distance ($mm$)
-   LBL: lower bract length ($mm$)
-   LBW: lower bract width ($mm$)
-   UBL: upper bract length ($mm$)
-   UBW: upper bract width ($mm$)
-   GW: gland width ($mm$)
-   GA: gland area ($mm^2$)

The traits have known or assumed functions. Anther-stigma distance is important for the ability of self-pollination, gland-anther distance and gland-stigmas distance affect the fit of flowers to pollinators, the upper and lower bracts are advertisements (think petals in other flowers), and the gland produces the the reward for pollinators.

The first step in any data analysis in always to explore the data. Make a series of histograms and plots. How are the data distributed? Are there any problematic outliers? How are patterns of trait correlations? Which traits are (proportionally) more variable?

What about differences between populations? How different are the trait means? Are any of the traits detectably different? To get started, the following lines read the data.

```{r}
blossoms = read.csv(datasets/blossoms.csv)
as.data.frame(blossoms)
names(blossoms)
```

To summarize the data per population, the `apply` family of functions are useful. To call a function for each level of a factor, such as computing the mean for each population, we can use `tapply`.

```{r}
tapply(blossoms$UBW, blossoms$pop, mean, na.rm=T)
```

A couple of packages are also very useful for producing complete summaries. I use `plyr` and `reshape2`. You could also consider learning some of the more modern things such as `tidyverse`.

```{r}
library(plyr)
library(knitr)
popstats = ddply(blossoms, .(pop), summarize,
                 LBWm = mean(LBW, na.rm=T),
                 LBWsd = sd(LBW, na.rm=T),
                 GSDm = mean(GSD, na.rm=T),
                 GSDsd = sd(GSD, na.rm=T),
                 ASDm = mean(ASD, na.rm=T),
                 ASDsd = sd(ASD, na.rm=T))
popstats[,-1] = round(popstats[,-1], 2)
kable(popstats)
```

After exploring and summarizing the data, fit some linear models to estimate the slopes of one trait on another. Interpret the results. Do the analysis on both arithmetic and log scale. Choose traits that belong to the same vs. different functional groups, can you detect any patterns in the slopes? Produce tidy figures that illustrate the results. Hint: once you have produced a scatterplot, you can add more points (e.g. for a different variable) by using the `points()` function.
