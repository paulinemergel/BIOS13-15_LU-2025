---
title: "Linear Models (BIOS15)"
author: "Pauline"
format: html
---

# 2025-11-12_Ch. 3_ANOVA

## Packages

```{r}
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("effects")
install.packages("ggpubr")
install.packages("Rmisc")

library(tidyverse)
library(ggplot2)
library(effects)
library(ggpubr)
library(Rmisc)
```

------------------------------------------------------------------------

## Overview

= Analysis of Variance

When our predictor variables are categorical (factors), linear models are used to perform analyses of variance. The parameter estimation works in much the same way as in regression, except that instead of estimating regression slopes, we are estimating group effects.

The aim of our ANOVA analysis is to partition the variance in our response variable into a set of additive components (recall, variances are additive). Based on this, we can evaluate whether the variance among groups is greater than the variance within groups, or more so than expected by chance.

The ANOVA framework is based on calculating "sums of squares" (SS), the sum of the squared deviations of each group mean from the grand mean (the variance among groups), and of each datapoint from either the grand mean (which is equal to the total variance in the data), or the group means (the residual or within-group variance).

```{r, fig.height=4, fig.width=8, fig.cap="Illustration of total, among-groups, and within-groups sums of squares. The total sum of squares is the total variance in the data, which can be partitioned into among-group and within-group (residual) components", echo=F}
set.seed(29)
groups = as.factor(rep(c("a", "b", "c"), each=5))
y = c(rnorm(5, 10, 3), rnorm(5, 13, 3), rnorm(5, 14, 3))
means = tapply(y, groups, mean)
xvals = as.numeric(groups)+rep(seq(-.15, .15, length.out=5), 3)

par(mfrow=c(1,2), mar=c(1,1,2,1))
plot(xvals, y, las=1,
     xlim=c(0.5, 3.5),
     xaxt="n", xlab="",
     yaxt="n", ylab="",
     main="Total and among-groups SS")
abline(h=mean(y), lty=1)
segments(c(.8, 1.8, 2.8), means, c(1.2, 2.2, 3.2), lwd=2)
segments(1:3, mean(y), 1:3, means, lwd=2)
segments(xvals, mean(y), xvals, y, lty=2)

plot(xvals, y, las=1,
     xlim=c(0.5, 3.5),
     xaxt="n", xlab="",
     yaxt="n", ylab="",
     main="Within-groups SS")
segments(c(.8, 1.8, 2.8), means, c(1.2, 2.2, 3.2), lwd=2)
segments(xvals, rep(means, each=5), xvals, y, lty=2)
```

------------------------------------------------------------------------

## Assumptions

Assumptions of a two-way ANOVA are similar than for a one-way ANOVA. To summarize:

-   **Variable type**: the dependent variable must be quantitative continuous, while the two independent variables must be categorical (with at least two levels).

-   **Independence**: the observations should be independent between groups and within each group.

-   **Normality**:

    -   For small samples, data should follow approximately a normal distribution

    -   For large samples (usually n≥30 in each group/sample), normality is not required (thanks to the central limit theorem)

-   **Equality of variances**: variances should be equal across groups.

-   **Outliers**: There should be no significant outliers in any group.

------------------------------------------------------------------------

## Basic Plot

To understand how the ANOVA analysis works, let's simulate some data, fit a linear model, and perform an ANOVA.

```{r}
set.seed(100)
groups = as.factor(rep(c("Low", "Medium", "High"), each=50)) 
x = c(rnorm(50, 10, 3), rnorm(50, 13, 3), rnorm(50, 14, 3))

plot(groups, x, las=1, xlab="", ylab="Body size (g)")
```

Plots like this are called "boxplots", and can be useful for visualising the distribution of data across factor levels or other groups. With the default settings, the boxes range from the 1st to the 3rd quartile, i.e. they span 50% of the data. The thick lines show the median, the "whiskers" extend to 1.5 times the inter-quartile range, and individual circles show outliers. This representation allows us to assess whether the data are roughly normally distributed within each group, which will tend to yield normally distributed residuals within each group, an assumption of the ANOVA model.

Boxplots are not directly useful for discussing ANOVA and variance partitioning though, and plots that show all the data can be more informative. Here is a rather elaborate plot combining a scatterplot (with values slightly "jittered" along the x-axis for clarity) and a boxplot.

EXTRA EXERCISE: Reproduce a plot similar to this (see page 3 in Lecture Notes Ch. 3) for the ANOVA exercise.

```{r}
## jitter()

par(pty = "s")  ## makes the graph SQUARE
plot(groups, x, col = 'lightgrey', pch = 1, ylab = "Body size (g)", xlab = "", )
points(groups, jitter(x, factor = 5) ,col='lightgrey')#, pch=3, cex=4) # points do not move
```

The r-code below provides the SOLUTION for the graph that combines boxplot, data points, and the mean of each group into one plot.

tapply() jitter()

```{r, fig.width=4, fig.height=4, echo=F}
plot(as.numeric(groups) + rnorm(150,0,0.03), x, 
     las=1, xlab="", ylab="Body size (g)",
     type="p", col="grey",
     xlim=c(0.5, 3.75), xaxt="n", yaxt="n")
axis(1, 1:3, labels=levels(groups))

# the lines above (by themselves at least) do NOT plot anything
# WHY?

means = tapply(x, groups, mean)
means
points(1:3, means, pch=16, col="black")

par(new=T)
plot(groups, x, at=c(1.3, 2.3, 3.3), boxwex=0.3, 
     xlim=c(0.5, 3.75), xaxt="n",
     las=1, xlab="", ylab="")

# at=c(1.3, 2.3, 3.3) specifies where along the x-axis each box should be placed
# boxwex = 0.3 controls the width of the boxes - the default is 0.8, so this makes the boxes narrower
# xaxt = "n" means not to draw the x-axis (the tick marks and labels)
# las controls the orientation of the axis labels: las = 1 means all labels are horizontal

          ## REVIEW THIS CODE 
```

------------------------------------------------------------------------

## 1-way ANOVA

Just as for linear regression, we fit the model with the `lm` function and save the results to an object (here `m`). To perform an ANOVA based on the fitted model, we call the `anova` function.

#### Model

```{r}
m = lm(x~groups)
```

#### ANOVA Table

```{r}
anova(m)
```

The [**ANOVA table**]{.underline} contains a lot of information. First, we learn about the number of **degrees of freedom** for each variable. For the `groups` variable (our focal factor), the 2 degrees of freedom is the number of groups in our data (3) - 1. The minus 1 comes from the fact that we had to estimate the mean in the data to obtain our sums of squares (the sum of the square deviations of data points from their group means). Similarly for residual degrees of freedom, we have 150 - 2 - 1, where the 2 comes from estimating the two contrasts (difference of group 2 and 3 from group 1), and the 1 is still the estimated mean.

The `Sum Sq` are the **sums of square**s, i.e. the sum of the squared deviations of each observation from the grand mean. The total sum of squares ($SS_T$) divided by $n-1$ gives the **total variance of the sample**. Call the `var` function to confirm that this is the case.

```{r}
SS_T = 319.97+1200.43
SS_T/(150-1)
```

```{r}
var(x)
```

We can easily get the **proportion of variance explained by the `groups` variable**, which is the same as the $r^2$ for the model.

```{r}
# R^2 = sum of squares of the groups/sum of squares total (groups + residuals)
319.97/SS_T
```

The `Mean Sq` is the variance attributable to each variable, conventionally called the **mean sum of squares** (the sum of squares divided by the degrees of freedom). The **F-ratio** (the test statistic in an ANOVA) is computed as the mean sum of squares for the focal variable divided by the mean residual sum of squares. Thus, it represents the ratio of the among-group variance to the within-group variance, but also the sample size which gives the residual degrees of freedom and thus, all else being equal, larger sample size gives lower mean sum of squares and thus higher F-ratios and lower $P$-values.

In an ANOVA, a statistically supported among-group variance component such as the one above indicates that at least one group mean is different from the others. To further assess which groups are different, we can extract the typical summary table of the linear model.

#### Summary Table

```{r}
summary(m)
```

This contains some of the same information as the ANOVA table, but we now also obtain parameter estimates. The first parameter, the **intercept**, corresponds to the estimated mean for the first level of the `groups` factor. In this example this happens to be 'High', because H comes before L and M in the alphabet. The **next two estimates represents *contrasts* from the reference group**, and the associated hypothesis tests test the **null hypothesis** that the group has the same mean as the reference group.

The summary table also gives us directly the $r^2$, which is a simple ANOVA is defined as $1-SS_E/SS_T$, where $SS_E$ is the sum of squares for the error term (residuals), and $SS_T$ is the total sum of squares. (Control question: why could we do it even more simply above?).

We calculated the $r^2$ above (see ANOVA table) already as the sum of squares of the groups divided by the total sum of squares.

```{r}
319.97/SS_T
```

The parameter estimates allow us to quantify the **effect size**, i.e. the magnitude of the difference between the groups. A useful way to report such differences is to **compute the % difference** (the contrast (= intercept = group "high") divided by the mean of the reference group "low", here $-3.456/13.701 = -0.252$), so that we can say that "Individuals in the low-food treatment were 25.2% smaller than individuals in the high-food treatment".

Note that if we want a **different reference group**, we can change the order of the factor levels.

```{r}
groups = factor(groups, levels=c("Low", "Medium", "High"))
m = lm(x~groups)
summary(m)
```

Sometimes we also want to **suppress the intercept** of the model, and thus estimate the mean and standard error for each level of the predictor. We can do this by adding `-1` to the model formula (what comes after the `~` sign). This could be useful for example, if we wanted **to obtain the estimated mean for each group**, associated for example with a **95% confidence interval**.

```{r}
m = lm(x~groups-1)
summary(m)$coef
confint(m)
```

------------------------------------------------------------------------

## 2- way ANOVA

Analyses of variance can also be performed with more than one factor variable. If we have two factors, we can talk about two-way ANOVA, and so on. The **two-way ANOVA (analysis of variance) is a statistical method that allows to evaluate the simultaneous effect of two categorical variables on a quantitative continuous variable.**

A typical example from biology is when we have performed a factorial experiment, and want to assess the effects of each experimental factor and their potential interaction.

With two factors, a full model can be formulated as `y ~ factor1 * factor2`. Recall that in `R` syntax, the \* means both main effects and their interaction, while a : means only the interaction. A detectable interaction term in this model would indicate that the effect of factor 1 depends on the level of factor 2 (and *vice versa*). If we are analysing an experiment where we have manipulated both temperature and nitrogen supply, an interaction would mean that the effect of temperature depends on the nitrogen level.

In a two-way ANOVA, you have:

-   Two categorical independent variables (factors)

-   One numeric dependent variable

You test:

-   Main effect of Factor A — Does A affect the outcome?

-   Main effect of Factor B — Does B affect the outcome?

-   Interaction A × B — Does the effect of A depend on B (or vice versa)?

Formula:

-   `dependent variable ~ independent variables`

-   the `+` sign is used to include independent variables *without* an interaction

-   the `*` sign is used to include independent variables *with* an interaction

### Exercise: analysing a factorial experiment

The following data are from an experiment where female butterflies reared on two different host plants were allowed to oviposit on the same two host plants. The data include developmental time for the larvae, the adult weight, and the growth rate of the larvae.

Analyse the data to assess effects of maternal vs. larval host plant on one or more response variable. Interpret the results and produce a nice plot to illustrate.

#### Data (Butterflies)

```{r}
dat = read.csv("datasets/butterflies.csv")
names(dat)

print(dat)
unique(dat$LarvalHost)
unique(dat$MaternalHost)
unique(dat$DevelopmentTime)
```

As a first step, let us compute some summary statistics, like the mean development time for each combination of larval and maternal host plant.

```{r}
## paste0( )
## is used to concatenate (join) strings together — without any spaces between them
## it’s a simplified version of the more general paste() function.


      ### ONLY RUN ONCE!!! 
      ### because it modifies (adds to) column names each time you run it


dat$MaternalHost = paste0(dat$MaternalHost, "MH")
dat$LarvalHost = paste0(dat$LarvalHost, "LH")
unique(dat$LarvalHost)
```

```{r}
larval.Brea <- dat %>% 
  select(DevelopmentTime, LarvalHost) %>% 
  filter(LarvalHost == "BarbareaLH")
larval.Brea

# Kernel density estimation
d.all <- density(dat$DevelopmentTime)
d.larval <- density(larval.Brea$DevelopmentTime)

# Kernel density plot
plot(d.all, lwd = 2, main = "Kernel density plot of Development Time (all data)")
plot(d.larval, lwd = 2, main = "Kernel density plot of Development Time (only larval plant Barbarea")

    ### Do we test for NORMALITY on the raw data and/or the model??
```

#### Model

```{r}
m2 <- lm(DevelopmentTime ~ LarvalHost * MaternalHost, data = dat)

## NORMALITY
# QQplot to test for normality
# method 1
plot(m2, which = 2)

# method 2
# requires package "car"
qqPlot(m2$residuals,
  id = FALSE # remove point identification
)

# Histogram
hist(m2$residuals)


## HOMOGENITY OF VARIANCES
plot(m2, which = 3)
```

##### [Assumptions met?]{.underline}

-   independent variables are categorical, dependent variable is continuous

-   we can assume independence within and between groups

-   normality? our data is skewed

    -   BUT with a sample size of 180 per group (so \>30/group) =\> central limit theorem applies. Thus, normality assumption does not apply

-   are variances equal across groups: line is flattish, we should be OK

-   we had some, but no significant outliers in any group

##### [First findings]{.underline}

-   Factor one is the larval host plant

-   Factor two is the maternal host plant

-   Development time is the numerical outcome

We would like to know if development time depends on larval host plant and/or maternal host plant.

So we:

1.  measure and test relationship larval host plant X development time

2.  measure and relationship maternal host plant X development time

3.  check whether the relationship between maternal host plant and development time is different for different larval host plants

=\> 1.) and 2.) are the main effects and 3.) is known as the interaction effect

The main effects test whether at least one group is different from another one (while controlling for the other independent variable). On the other hand, the interaction effect aims at testing whether the relationship between two variables differs *depending on the level of a third variable*.

Before we even run the model, we can see that the effect of the maternal host plant on the development of the butterfly is different depending on the larval host plant. There appears to be an interaction. So let's move on with the model.

#### Descriptive Stats

```{r}
# MEAN (method 1)
means = tapply(dat$DevelopmentTime, list(dat$MaternalHost, dat$LarvalHost), mean)
means

# MEAN and SD (method 2)
# library(dplyr)

group_by(dat, LarvalHost, MaternalHost) %>%
  summarise(
    mean = round(mean(DevelopmentTime, na.rm = TRUE)),
    sd = round(sd(DevelopmentTime, na.rm = TRUE))
  )



#labelsx <- c(unique(dat$MaternalHost), unique(dat$LarvalHost))
#plot(1:4, means, pch=16, las = 1, col = "red")


# Basic R boxplot
boxplot(DevelopmentTime ~ LarvalHost * MaternalHost, data = dat,
        cex.axis = 0.5, 
        las = 1, xlab = "", ylab = "Development Time"
        )

# ggplot2 boxplot
# boxplot by group
ggplot(dat) +
  aes(x = LarvalHost, y = DevelopmentTime, fill = MaternalHost) +
  geom_boxplot()


```

#### ANOVA Table

How to read/approach the ANOVA table:

Similar to a one-way ANOVA, the principle of a two-way ANOVA is based on the total dispersion of the data, and its decomposition into four components:

1.  the share attributable to the first factor

2.  the share attributable to the second factor

3.  the share attributable to the interaction of the 2 factors

4.  the unexplained, or residual portion.

The sum of squares (column `Sum Sq`) shows these four components. The two-way ANOVA consists of using a statistical test to determine whether each of the dispersion component (attributable to the 2 factors studied and to their interaction) is significantly greater than the residual component. If this is the case, we conclude that the effect considered (factor A, factor B or the interaction) is significant.

```{r}
## ANOVA Table
anova(m2)
aov(m2)

## Total variance in the data
(2809.15+496.87)/(nrow(dat)-1)
var(dat$DevelopmentTime)
```

##### Results I

(Degrees of freedom == How many ways can the data vary?)

Is the among group variance ("groups") greater or smaller or similar to the within group variance ("residuals")?

**F-value**

How much of the variance in data is explained by the groups\
= sums of squares of groups / total sums of squares\
\~ r\^2 (see summary ( ))

We see that the species explain a large part of the variability of body mass. It is the most important factor in explaining this variability.

1.  Controlling for the maternal host, developmental time is "significantly" different between the two larval host plants

2.  Controlling for the larval host, developmental time is also "significantly" different between the two maternal host plants

3.  However, also the interaction larval host plant X maternal host plant is "significantly" different

In other words, a large sum of squares of the interaction indicates that the effects of the factors are not simply additive, and the two factors interact with each other in a meaningful way.

Therefore, we do NOT have to modify the model (remove the interaction). We keep everything in the model as is.

| Column | Meaning |
|----|----|
| **Df** | Degrees of freedom for that factor (each factor with 2 levels → 1 df). |
| **Sum Sq** | The sum of squares attributable to that source — the “variation explained” by that effect. |
| **Mean Sq** | The average sum of squares (Sum Sq ÷ Df). |
| **F value** | Test statistic comparing model variance (for that factor) to residual variance. |
| **Pr(\>F)** | p-value — probability that the observed F is due to chance (under H₀: no effect). |

| Row | Interpretation |
|----|----|
| **LarvalHost** | Tests whether mean DevelopmentTime differs between larvae reared on *Barbarea* vs *Berteroa* **on average**, ignoring maternal host. |
| **MaternalHost** | Tests whether mean DevelopmentTime differs between offspring from mothers fed on *Barbarea* vs *Berteroa*, **on average**, ignoring larval host. |
| **LarvalHost:MaternalHost** | Tests whether the **effect of larval host depends on the maternal host** — i.e., whether the difference between hosts changes depending on the mother’s host. This is the *interaction*. |
| **Residuals** | Unexplained variance — how much variation remains after accounting for those effects. |

In your data, all three terms were significant in the regression summary — so in the ANOVA table you’ll also see:

-   **LarvalHost: p \< 0.001** → Strong evidence that larval host species affects development time.

-   **MaternalHost: p \< 0.001** → Maternal host also has a significant effect.

-   **Interaction: p \< 0.001** → The effect of larval host depends on maternal host (not parallel differences).

A two-way ANOVA revealed significant effects of **larval host** (F₁,₂₈₃ ≈ …, p \< 0.001) and **maternal host** (F₁,₂₈₃ ≈ …, p \< 0.001) on development time.\

There was also a **significant interaction** between larval and maternal host (F₁,₂₈₃ ≈ …, p \< 0.001), indicating that the effect of larval host on development time depended on the maternal host species.

anova(model) versus summary(model):

-   ANOVA (from `anova(model)`) tells you **which effects are significant**.

-   The **`summary(model)`** tells you **how large** and **in what direction** those effects are (the estimates).

-   Together, they tell the full story:

    -   *Both larval and maternal hosts significantly affect development time.*

    -   *The combination of both Berteroa hosts leads to the longest development time.*

#### Summary Table

```{r}
## estimates
summary(m2)

## group means
model.tables(aov(m2), type = "means")

## r2
```

##### Results II

The S.E. is (relatively) small.

The intercept is the mean or expected value of the reference group (alphabetically first one). The other values are contrasts to the reference group.

=\> Take the difference to get the mean.

Intercept = **reference group is larval host Barabarea** has a mean of 21.7 days to develop.

It takes about 5 more days to develop on larval host Berteroa.

?? The developmental time on maternal host Berteroa is circa 23.5 days.

The interactive effect shows a difference of 2.2 days to the intercept.

$R^2 = 0.77$ =\> 77% of the variance in the data is explained by the groups.

| Term | Estimate | Meaning (in your context) |
|------------------------|------------------------|------------------------|
| (Intercept) | 21.6961 | Mean development time when **LarvalHost = Barbarea** *and* **MaternalHost = Barbarea** |
| LarvalHostBerteroaLH | +5.3039 | If larvae feed on **Berteroa** instead of Barbarea, development time increases by **≈5.30 days**, *when the maternal host was Barbarea*. |
| MaternalHostBerteroaMH | +1.8167 | If the **mother fed on Berteroa** instead of Barbarea, offspring development time increases by **≈1.82 days**, *when larvae are reared on Barbarea*. |
| LarvalHostBerteroaLH : MaternalHostBerteroaMH | +2.2025 | When **both** the larval and maternal hosts are Berteroa, development time increases by an **additional ≈2.20 days** beyond the sum of the two main effects. |

| LarvalHost | MaternalHost | Formula | Predicted Mean (days) |
|------------------|------------------|------------------|------------------|
| Barbarea | Barbarea | (Intercept) | **21.70** |
| Berteroa | Barbarea | (Intercept) + LarvalHostBerteroaLH | **21.70 + 5.30 = 27.00** |
| Barbarea | Berteroa | (Intercept) + MaternalHostBerteroaMH | **21.70 + 1.82 = 23.51** |
| Berteroa | Berteroa | (Intercept) + LarvalHostBerteroaLH + MaternalHostBerteroaMH + Interaction | **21.70 + 5.30 + 1.82 + 2.20 = 31.02** |

## Interpretation in Plain Language

-   **Baseline group:** Larvae and mothers both from *Barbarea* → \~21.7 days.

-   **Larval host effect:** Switching larvae to *Berteroa* (from Barbarea) slows development by \~5.3 days if the mother fed on Barbarea.

-   **Maternal host effect:** Offspring of *Berteroa* mothers (raised on Barbarea) take \~1.8 days longer to develop than those from Barbarea mothers.

-   **Interaction:** When both larval and maternal hosts are *Berteroa*, development is **even slower** — an extra \~2.2 days beyond the sum of the two effects.

    -   **Biological summary:**\
        Development is slowest (\~31 days) when both larval and maternal hosts are *Berteroa*. Both host species influence development time, and the combination magnifies the delay — a significant interaction.

#### Plot

```{r}
## Plot: larval host * maternal host effect plot

# see: https://statsandr.com/blog/two-way-anova-in-r/

# method 1
# library(effects)
plot(allEffects(m2))

# method 2
# libaryr = ? 
with(
  dat,
  interaction.plot(LarvalHost, MaternalHost, DevelopmentTime),
)

# method 3
# library(Rmisc)


# compute mean and standard error of the mean by subgroup
summary_stat <- summarySE(dat,
  measurevar = "body_mass_g",
  groupvars = c("species", "sex")
)

# plot mean and standard error of the mean
ggplot(
  subset(summary_stat, !is.na(sex)), # remove NA level for sex
  aes(x = species, y = body_mass_g, colour = sex)
) +
  geom_errorbar(aes(ymin = body_mass_g - se, ymax = body_mass_g + se), # add error bars
    width = 0.1 # width of error bars
  ) +
  geom_point() +
  labs(y = "Mean of body mass (g)")

## ADD ERROR BARS!!!

```

## Questions/Review

-   Ask if I can turn in the Cuarto script

-   Basic plot from slide with boxplot and points: Why does it only plot when you run the whole code? I don't quite grasp the code yet.

-   Do we test for normality on the raw data and/or the model?

-   Anova table: is it correct that we only have 1 df for larvalhost, maternalhost, larvalxmaternalhost

    -   I expected 2 df for maternalhost, larvalxmaternalhost

    -   Why not?!

-   How can I leave out "significant", if I look at the ANOVA table?
